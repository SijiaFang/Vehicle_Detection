{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"twitter/Corona_NLP_train.csv\",encoding='latin1')\n",
    "test = pd.read_csv(\"twitter/Corona_NLP_test.csv\",encoding='latin1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentiment to int\n",
    "def senti_to_int(s):\n",
    "    if s == 'Extremely Negative':\n",
    "        return 0\n",
    "    elif s == 'Negative':\n",
    "        return 1\n",
    "    elif s == 'Neutral':\n",
    "        return 2\n",
    "    elif s == 'Positive':\n",
    "        return 3\n",
    "    elif s == 'Extremely Positive':\n",
    "        return 4\n",
    "    \n",
    "y_train = [senti_to_int(s) for s in train['Sentiment']]\n",
    "y_test = [senti_to_int(s) for s in test['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/F970614/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_word = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean(text):\n",
    "    #remove urls\n",
    "    text = re.sub(r'http\\S+', \" \", text)\n",
    "    #remove mentions\n",
    "    text = re.sub(r'@\\w+',' ',text)\n",
    "    #remove hastags\n",
    "    text = re.sub(r'#\\w+', ' ', text)\n",
    "    #remove digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    #remove html tags\n",
    "    text = re.sub('r<.*?>',' ', text) \n",
    "    #remove stop words \n",
    "    text = text.split()\n",
    "    text = \" \".join([word for word in text if not word in stop_word])    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
      "advice Talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist GP set online shopping accounts poss adequate supplies regular meds order\n"
     ]
    }
   ],
   "source": [
    "print(train['OriginalTweet'][1])\n",
    "print(clean(train['OriginalTweet'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean(x) for x in train['OriginalTweet']]\n",
    "x_test = [clean(x) for x in test['OriginalTweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text to numeric vector\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advice Talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist GP set online shopping accounts poss adequate supplies regular meds order\n",
      "[439, 819, 2512, 199, 2632, 801, 1178, 1029, 362, 320, 801, 1178, 2512, 1161, 3509, 3248, 4522, 418, 13, 12, 2251, 3037, 118, 989, 2894, 165]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])\n",
    "print(sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 53)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pad_sequences(sequences_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798, 53)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pad_sequences(sequences_test, maxlen = 53)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.cast(x_train,tf.int32)\n",
    "x_test = tf.cast(x_test,tf.int32)\n",
    "y_train = tf.cast(y_train,tf.int32)\n",
    "y_test = tf.cast(y_test,tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingdim = 32\n",
    "model=keras.Sequential(\n",
    "        [   layers.InputLayer(input_shape=(41157,)),\n",
    "            layers.Embedding(v+1, embeddingdim),\n",
    "            layers.SimpleRNN(units = 64, activation = 'relu', return_sequences = True),\n",
    "            layers.GlobalMaxPool1D(),\n",
    "            layers.Dense(32,activation='relu'),\n",
    "            layers.Dense(5, activation=\"softmax\")\n",
    "        ])\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1287/1287 [==============================] - 35s 26ms/step - loss: 1.0329 - accuracy: 0.5809\n",
      "Epoch 2/4\n",
      "1287/1287 [==============================] - 35s 27ms/step - loss: 0.6040 - accuracy: 0.7926\n",
      "Epoch 3/4\n",
      "1287/1287 [==============================] - 33s 25ms/step - loss: 0.4903 - accuracy: 0.8396\n",
      "Epoch 4/4\n",
      "1287/1287 [==============================] - 34s 27ms/step - loss: 0.4059 - accuracy: 0.8676\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 1s - loss: 0.7533 - accuracy: 0.7420\n",
      "0.7419694662094116\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, ..., 2, 0, 4])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_predict, axis=1, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'predict':np.argmax(y_predict, axis=1, out=None),\n",
    "            \"true\":y_test,\n",
    "            \"text\":test['OriginalTweet']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>true</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predict  true                                               text\n",
       "0        0     0  TRENDING: New Yorkers encounter empty supermar...\n",
       "1        3     3  When I couldn't find hand sanitizer at Fred Me...\n",
       "2        4     4  Find out how you can protect yourself and love...\n",
       "3        1     1  #Panic buying hits #NewYork City as anxious sh...\n",
       "4        2     2  #toiletpaper #dunnypaper #coronavirus #coronav..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingdim = 32\n",
    "model1=keras.Sequential(\n",
    "        [   layers.InputLayer(input_shape=(41157,)),\n",
    "            layers.Embedding(v+1, embeddingdim),\n",
    "            layers.LSTM(units = 64, activation = 'relu', return_sequences = True),\n",
    "            layers.GlobalMaxPool1D(),\n",
    "            layers.Dense(32,activation='relu'),\n",
    "            layers.Dense(5, activation=\"softmax\")\n",
    "        ])\n",
    "model1.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1287/1287 [==============================] - 66s 50ms/step - loss: 1.3510 - accuracy: 0.4742 - val_loss: 0.9721 - val_accuracy: 0.6145\n",
      "Epoch 2/4\n",
      "1287/1287 [==============================] - 56s 44ms/step - loss: 0.7805 - accuracy: 0.7374 - val_loss: 0.7989 - val_accuracy: 0.7088\n",
      "Epoch 3/4\n",
      "1287/1287 [==============================] - 55s 43ms/step - loss: 0.5801 - accuracy: 0.8066 - val_loss: 0.7910 - val_accuracy: 0.7167\n",
      "Epoch 4/4\n",
      "1287/1287 [==============================] - 1150s 894ms/step - loss: 0.4868 - accuracy: 0.8411 - val_loss: 0.7807 - val_accuracy: 0.7401\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(x_train, y_train, epochs=4, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 - 1s - loss: 0.7807 - accuracy: 0.7401\n",
      "0.740126371383667\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model1.evaluate(x_test, y_test, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    ## rate: drop out rate\n",
    "    ## embed dim: d_k in multihead attention \n",
    "    ## ff_dim: hidden layer size in feed forward network \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # multihead attention layer+drop out\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # residual connection+normalization\n",
    "        out1 = self.layernorm1(inputs + attn_output)       \n",
    "        # feed forward+drop out\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # residual connection+normalization\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "# embedding+position    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([41157, 53])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 \n",
    "num_heads = 6\n",
    "ff_dim = 32  \n",
    "maxlen = 53\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, v+1, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1287/1287 [==============================] - 84s 64ms/step - loss: 0.9565 - accuracy: 0.6135 - val_loss: 0.7450 - val_accuracy: 0.7314\n",
      "Epoch 2/3\n",
      "1287/1287 [==============================] - 1164s 905ms/step - loss: 0.5649 - accuracy: 0.8001 - val_loss: 0.6878 - val_accuracy: 0.7483\n",
      "Epoch 3/3\n",
      "1287/1287 [==============================] - 81s 63ms/step - loss: 0.4815 - accuracy: 0.8367 - val_loss: 0.6887 - val_accuracy: 0.7583\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Transformer seems to be better, but not to much. But it runs faster (great!)\n",
    "\n",
    "- RNN and LSTM aren't too different, RNN even seems to be slightly better.\n",
    "\n",
    "- Possible reason: sequence isn't that long, max_len = 53 in this case. Also, transformer are designed for seq2seq, here we drop the decoder layer to perform classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
